// Copyright (C) 2015 Technische Universitaet Muenchen
// This file is part of the Mamico project. For conditions of distribution
// and use, please see the copyright notice in Mamico's main folder, or at
// www5.in.tum.de/mamico

#include "SendReceiveBuffer.h"

template<class MacroscopicCell,unsigned int dim>
coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::SendReceiveBuffer(){
  deleteBuffers();
  #if (COUPLING_MD_PARALLEL==COUPLING_MD_YES)
  _requestsAllocated=false;
  _requests=NULL;

  MPI_Op_create(&elementWiseSum, 1, &elementWiseSumOperation);
  #endif
}




template<class MacroscopicCell,unsigned int dim>
coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::SendReceiveBuffer::~SendReceiveBuffer(){
  deleteBuffers();
  #if (COUPLING_MD_PARALLEL==COUPLING_MD_YES)
  if (_requests!= NULL){delete []_requests; _requests = NULL;}
  for(MPI_Comm & subcomm : _subComms) {
    if(subcomm != MPI_COMM_NULL) { MPI_Comm_free(&subcomm); subcomm = MPI_COMM_NULL; }
  }
  for(MPI_Group & subgroup : _subGroups) {
    if(subgroup != MPI_GROUP_NULL) { MPI_Group_free(&subgroup); subgroup = MPI_GROUP_NULL; }
  }
  _subComms.clear();
  _subGroups.clear();

  MPI_Op_free(&elementWiseSumOperation);
  #endif
}



template<class MacroscopicCell,unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::
deleteBuffer(std::map<unsigned int,BufferWithID>& buffer){
  for (typename std::map<unsigned int,BufferWithID>::iterator it = buffer.begin(); it != buffer.end(); it++){
    if ( (it->second).buffer != NULL){
      free( (it->second).buffer );
      (it->second).buffer = NULL;
    }
    (it->second).bufferSize = 0;
  }
  buffer.clear();
}



template<class MacroscopicCell,unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::deleteBuffers(){
  deleteBuffer(_sendBuffer);
  deleteBuffer(_receiveBuffer);
  _bcastBuffer.clear();
  _reduceBuffer.clear();
}


template<class MacroscopicCell, unsigned int dim>
  void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::writeToBcastBuffer(
  const coupling::IndexConversion<dim>& indexConversion,
  coupling::sendrecv::DataExchange<MacroscopicCell,dim> &dataExchange,
  const MacroscopicCell& macroscopicCell,
  tarch::la::Vector<dim,unsigned int> globalVectorIndex
) {
  const unsigned int doublesPerCell = dataExchange.getDoublesPerCell()+1;

  const unsigned int globalIndex = indexConversion.getGlobalCellIndex(globalVectorIndex);

  unsigned int intNumberProcesses = indexConversion.getNumberProcesses()[0];
  for(unsigned int d = 1; d < dim ; ++d) {
    intNumberProcesses *= indexConversion.getNumberProcesses()[d];
  }
  // determine the ranks which need to receive information of this macroscopic cell
  const std::vector<unsigned int> ranks = dataExchange.getTargetRanks(globalVectorIndex);

  if(ranks.empty()) {
    return;
  }

  // determine unique subdomains which need to receive this information
  // and associate them with targetRanks
  std::map<unsigned int, std::set<unsigned int> > subdomainMap;
  for(const unsigned int & rank : ranks) {

    unsigned int subdomain = rank % intNumberProcesses;
    unsigned int key = indexConversion.getThisRank() * intNumberProcesses + subdomain;

    subdomainMap[key].insert(rank);
  }

  // collect data for subdomains
  for(std::pair<const unsigned int, std::set<unsigned int>> & subdomain : subdomainMap) {
    unsigned int key = subdomain.first;

    // create new buffer if non-existent and insert
    // information about ranks
    _bcastBuffer[key].rootRank = indexConversion.getThisRank();
    _bcastBuffer[key].nonRootRanks.insert(subdomain.second.begin(), subdomain.second.end());

    if(_bcastBuffer[key].cellIndices.find(globalIndex) != _bcastBuffer[key].cellIndices.end()) {
      // buffer already contains data of cell, nothing to do!
      continue;
    }
    _bcastBuffer[key].cellIndices.insert(globalIndex);

    // resize buffer for additional data
    const unsigned int firstPos = _bcastBuffer[key].buffer.size();
    _bcastBuffer[key].buffer.resize(_bcastBuffer[key].buffer.size() + doublesPerCell);

    // write global cell index and call to specialised function to read from macroscopic cell and write to send buffer
    _bcastBuffer[key].buffer[firstPos] = (double) globalIndex;

    dataExchange.readFromCell(&_bcastBuffer[key].buffer.data()[firstPos + 1], macroscopicCell );
  }
}


template<class MacroscopicCell,unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::writeToSendBuffer(
  const coupling::IndexConversion<dim>& indexConversion,
  coupling::sendrecv::DataExchange<MacroscopicCell,dim> &dataExchange,
  const MacroscopicCell& macroscopicCell,
  tarch::la::Vector<dim,unsigned int> globalVectorIndex
){
  const unsigned int doublesPerCell = dataExchange.getDoublesPerCell()+1;

  const unsigned int globalIndex = indexConversion.getGlobalCellIndex(globalVectorIndex);

  // determine the ranks which need to receive information of this macroscopic cell
  const std::vector<unsigned int> ranks = dataExchange.getTargetRanks(globalVectorIndex);
  /*if(indexConversion.getThisRank() == 0) {
    std::cout << ranks << std::endl;
  }*/
  const unsigned int ranksSize = (unsigned int) ranks.size();
  #if (COUPLING_MD_DEBUG==COUPLING_MD_YES)
  std::cout << "Rank " << indexConversion.getThisRank() << ": write cell " << globalVectorIndex << " to send buffer;";
  std::cout << " associated target ranks: ";
  for (unsigned int i = 0; i < ranksSize; i++){ std::cout << ranks[i] << " ";} std::cout << std::endl;
  #endif

  // loop over ranks
  for (unsigned int i = 0; i < ranksSize; i++){

    // get iterator to map
    typename std::map<unsigned int,BufferWithID>::iterator thisBuffer = _sendBuffer.find(ranks[i]);
    if (thisBuffer== _sendBuffer.end()){
      _sendBuffer[ranks[i]] = BufferWithID();
      thisBuffer = _sendBuffer.find(ranks[i]);
    }

    // increment buffer size and realloc buffer
    const unsigned int firstPos = (thisBuffer->second).bufferSize*doublesPerCell;
    (thisBuffer->second).bufferSize++;
    (thisBuffer->second).buffer = (double *) realloc( (thisBuffer->second).buffer, sizeof(double)*(thisBuffer->second).bufferSize*doublesPerCell );
    if ( (thisBuffer->second).buffer == NULL){
      std::cout << "ERROR coupling::SendReceiveBuffer::writeToSendBuffer(): realloc yields NULL ptr!" << std::endl;
      exit(EXIT_FAILURE);
    }

    // write global cell index and call to specialised function to read from macroscopic cell and write to send buffer
    (thisBuffer->second).buffer[firstPos] = (double) globalIndex;

    dataExchange.readFromCell( &((thisBuffer->second).buffer[firstPos+1]), macroscopicCell );
  } // rank
}




template<class MacroscopicCell,unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::readFromReceiveBuffer(
  const coupling::IndexConversion<dim> &indexConversion,
  coupling::sendrecv::DataExchange<MacroscopicCell,dim> &dataExchange,
  MacroscopicCell &macroscopicCell,
  tarch::la::Vector<dim,unsigned int> globalVectorIndex
){
  const unsigned int doublesPerCell = dataExchange.getDoublesPerCell()+1;
  const unsigned int globalIndex = indexConversion.getGlobalCellIndex(globalVectorIndex);

  #if (COUPLING_MD_DEBUG==COUPLING_MD_YES)
  std::cout << "Rank " << indexConversion.getThisRank() << ": read cell " << globalVectorIndex << " (" << globalIndex << ") " << " from receive buffer" << std::endl;
  #endif

  // loop over all received buffers
  for (typename std::map<unsigned int, BufferWithID>::const_iterator it = _receiveBuffer.begin(); it != _receiveBuffer.end(); it++){

    // loop over all received cells of this buffer
    const unsigned int bufferSize = (it->second).bufferSize;
    for (unsigned int i = 0; i < bufferSize; i++){

      // determine global cell index for this macroscopic cell
      const unsigned int thisIndex = (unsigned int) (it->second).buffer[i*doublesPerCell];

      // if this is the correct cell, write data into macroscopic cell and return
      if (thisIndex == globalIndex){
        // read receive buffer and write to macroscopic cell (use specialised function)
        dataExchange.writeToCell( &((it->second).buffer[i*doublesPerCell+1]), macroscopicCell );
        return;
      }
    } // loop over buffer
  } // loop over all buffers
}



template<class MacroscopicCell,unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::readFromCollectiveBuffer(
const coupling::IndexConversion<dim> &indexConversion,
coupling::sendrecv::DataExchange<MacroscopicCell,dim> &dataExchange,
MacroscopicCell &macroscopicCell,
tarch::la::Vector<dim,unsigned int> globalVectorIndex
){
  const unsigned int doublesPerCell = dataExchange.getDoublesPerCell()+1;
  const unsigned int globalIndex = indexConversion.getGlobalCellIndex(globalVectorIndex);

#if (COUPLING_MD_DEBUG==COUPLING_MD_YES)
  std::cout << "Rank " << indexConversion.getThisRank() << ": read cell " << globalVectorIndex << " (" << globalIndex << ") " << " from receive buffer" << std::endl;
#endif

  // loop over all received buffers
  for (typename std::map<unsigned int, BufferCollective>::const_iterator it = _bcastBuffer.begin(); it != _bcastBuffer.end(); it++){

    // loop over all received cells of this buffer
    const unsigned int bufferSize = it->second.buffer.size();
    for (unsigned int i = 0; i < bufferSize; i+=doublesPerCell){

      // determine global cell index for this macroscopic cell
      const auto thisIndex = (unsigned int) it->second.buffer[i];

      // if this is the correct cell, write data into macroscopic cell and return
      if (thisIndex == globalIndex){
        // read receive buffer and write to macroscopic cell (use specialised function)
        dataExchange.writeToCell( &(it->second.buffer.data()[i+1]), macroscopicCell );
        return;
      }
    } // loop over buffer
  } // loop over all buffers
}



template<class MacroscopicCell,unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::allocateReceiveBuffers(
  const coupling::IndexConversion<dim> &indexConversion,
  coupling::sendrecv::DataExchange<MacroscopicCell,dim> &dataExchange,
  tarch::la::Vector<dim,unsigned int> globalVectorIndex
){
  const unsigned int doublesPerCell = dataExchange.getDoublesPerCell()+1;

  // determine the ranks from which we need to receive information of this macroscopic cell
  const std::vector<unsigned int> ranks = dataExchange.getSourceRanks(globalVectorIndex);
  const unsigned int ranksSize = (unsigned int) ranks.size();
  #if (COUPLING_MD_DEBUG==COUPLING_MD_YES)
  std::cout << "Rank " << indexConversion.getThisRank() << ": allocate receive buffers for cell " << globalVectorIndex << ";";
  std::cout << " associated source ranks: ";
  for (unsigned int i = 0; i < ranksSize; i++){ std::cout << ranks[i] << " ";} std::cout << std::endl;
  #endif


  // loop over ranks
  for (unsigned int i = 0; i < ranksSize; i++){

    // get iterator to map
    typename std::map<unsigned int,BufferWithID>::iterator thisBuffer = _receiveBuffer.find(ranks[i]);
    if (thisBuffer== _receiveBuffer.end()){
      _receiveBuffer[ranks[i]] = BufferWithID();
      thisBuffer = _receiveBuffer.find(ranks[i]);
    }

    // increment buffer size and realloc buffer
    const unsigned int firstPos = (thisBuffer->second).bufferSize*doublesPerCell;
    (thisBuffer->second).bufferSize++;
    (thisBuffer->second).buffer = (double *) realloc( (thisBuffer->second).buffer, sizeof(double)*(thisBuffer->second).bufferSize*doublesPerCell );
    if ( (thisBuffer->second).buffer == NULL){
      std::cout << "ERROR coupling::SendReceiveBuffer::allocateReceiveBuffers(): realloc yields NULL ptr!" << std::endl;
      exit(EXIT_FAILURE);
    }
    // set all values to -1.0
    for (unsigned int j = firstPos; j < firstPos+doublesPerCell; j++){
      (thisBuffer->second).buffer[j] = -1.0;
    }
  } // rank
}



template<class MacroscopicCell,unsigned int dim>
  void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::allocateBcastBufferForReceiving(
  const coupling::IndexConversion<dim> &indexConversion,
  coupling::sendrecv::DataExchange<MacroscopicCell, dim> &dataExchange,
  tarch::la::Vector<dim, unsigned int> globalVectorIndex
) {
  const unsigned int doublesPerCell = dataExchange.getDoublesPerCell()+1;


  // determine the ranks from which we need to receive information of this macroscopic cell
  const std::vector<unsigned int> sourceRanks = dataExchange.getSourceRanks(globalVectorIndex);
  const std::vector<unsigned int> targetRanks = dataExchange.getTargetRanks(globalVectorIndex);
  /*if(indexConversion.getThisRank() == 15) {
    std::cout << globalVectorIndex << " <---> " << indexConversion.getGlobalCellIndex(globalVectorIndex) << " => "
              << sourceRanks << " ==> " << targetRanks << std::endl;
  }*/
  if(sourceRanks.empty()) { return; }
#if (COUPLING_MD_DEBUG==COUPLING_MD_YES)
  std::cout << "Rank " << indexConversion.getThisRank() << ": allocate receive buffers for cell " << globalVectorIndex << ";";
  std::cout << " associated source ranks: ";
  for (unsigned int i = 0; i < ranksSize; i++){ std::cout << ranks[i] << " ";} std::cout << std::endl;
#endif

  unsigned int intNumberProcesses = indexConversion.getNumberProcesses()[0];
  for(unsigned int i = 1; i < dim ; ++i) {
    intNumberProcesses *= indexConversion.getNumberProcesses()[i];
  }

  const unsigned int globalIndex = indexConversion.getGlobalCellIndex(globalVectorIndex);

  unsigned int thisSubdomain = indexConversion.getThisRank() % intNumberProcesses;

  std::set<unsigned int> subdomainRanks;

  // Determine all receiving ranks of this subdomain
  for(const unsigned int & rank : targetRanks) {
    if(rank % intNumberProcesses == thisSubdomain) {
      subdomainRanks.insert(rank);
    }
  }
  if(subdomainRanks.empty()) { return; }

  // loop over source ranks
  for (const unsigned int & sourceRank : sourceRanks){
    if(sourceRank == indexConversion.getThisRank()) { continue; }
    unsigned int key = sourceRank * intNumberProcesses + thisSubdomain;

    _bcastBuffer[key].rootRank = sourceRank;
    _bcastBuffer[key].nonRootRanks.insert(subdomainRanks.begin(), subdomainRanks.end());

    if(_bcastBuffer[key].cellIndices.find(globalIndex) != _bcastBuffer[key].cellIndices.end()) {
      // space for this cell already allocated, nothing to do!
      continue;
    }
    _bcastBuffer[key].cellIndices.insert(globalIndex);

    // increment buffer size and resize buffer
    const unsigned int firstPos = _bcastBuffer[key].buffer.size();
    _bcastBuffer[key].buffer.resize(_bcastBuffer[key].buffer.size() + doublesPerCell);
    // set all values to -1.0
    for (unsigned int j = firstPos; j < firstPos+doublesPerCell; j++){
      _bcastBuffer[key].buffer[j] = -1.0;
    }
  } // rank
}



template<class MacroscopicCell,unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::
triggerSending(const coupling::IndexConversion<dim>& indexConversion,
coupling::sendrecv::DataExchange<MacroscopicCell,dim>& dataExchange){
  const unsigned int thisRank = indexConversion.getThisRank();
  const unsigned int doublesPerCell = dataExchange.getDoublesPerCell()+1;

  // loop over all ranks
  // counter loops over [0; _sendSize-1]
  #if (COUPLING_MD_PARALLEL==COUPLING_MD_YES)
  int counter = 0;
  #endif

  for (typename std::map<unsigned int,BufferWithID>::iterator it = _sendBuffer.begin(); it != _sendBuffer.end(); it++){
    // if this is not the same rank, do sending
    if ((it->first) != thisRank){
      #if (COUPLING_MD_PARALLEL==COUPLING_MD_YES)
      int err = MPI_Isend( (it->second).buffer,doublesPerCell*(it->second).bufferSize,MPI_DOUBLE,(it->first),dataExchange.getTag(),MPI_COMM_WORLD, &_requests[counter]);
      if (err != MPI_SUCCESS){
        char *str=NULL; int len=-1;
        MPI_Error_string(err,str,&len);
        std::cout << "ERROR coupling::sendrecv::SendRecvBuffer::triggerSending(): Sending from rank " << thisRank << " to " << it->first << " failed with error: " << str << std::endl;
        delete str;
        exit(EXIT_FAILURE);
      }
      counter++;
      #endif
    // otherwise: copy information to _receiveBuffer
    } else {

      // determine receive-buffer for this rank and check that the buffer is available and of correct size
      typename std::map<unsigned int,BufferWithID>::iterator itRecv = _receiveBuffer.find(thisRank);
      if ( (itRecv==_receiveBuffer.end()) ){
        std::cout << "ERROR coupling::SendReceiveBuffer::triggerSending(): Could not send from rank " << thisRank << " to the same rank! No receive buffer available!";
        std::cout << " Tag (in case of MPI; not used here): " << dataExchange.getTag() << std::endl;
        exit(EXIT_FAILURE);
      }
      if ( (itRecv->second).bufferSize != (it->second).bufferSize ){
        std::cout << "ERROR coupling::SendReceiveBuffer:triggerSending(): Send- and receive-buffer sizes do not match for information processed on same rank!" << std::endl;
        std::cout << "Recv-buffer size=" << (itRecv->second).bufferSize << ", send-buffer size=" << (it->second).bufferSize << std::endl;
        exit(EXIT_FAILURE);
      }

      // copy information from send- to receive buffer
      memcpy( (itRecv->second).buffer, (it->second).buffer, sizeof(double)*doublesPerCell*(it->second).bufferSize);
    }
  }
}



template<class MacroscopicCell, unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::triggerBcasts(
  const unsigned int thisRank
) {

#if (COUPLING_MD_PARALLEL==COUPLING_MD_YES)
  int counter = 0;
#endif
#if (COUPLING_MD_PARALLEL==COUPLING_MD_YES)
 // if(thisRank == 23) std::cout << thisRank << " sendrecvBuffer size " << _sendRecvBuffer.size() << std::endl;
  for(std::pair<const unsigned int, BufferCollective> & buffer : _bcastBuffer) {
    /*if(thisRank == 23) {
      std::cout <<thisRank << "=====" << buffer.second.cellIndices << "=====" <<buffer.second.sourceRank << "===>" << buffer.second.targetRanks << std::endl;
    }*/
    //std::cout << thisRank << " : Bcast buffer size " << buffer.second.buffer.size() << " from " << buffer.second.sourceRank << " to " << buffer.second.targetRanks << std::endl;
    int err = MPI_Ibcast(buffer.second.buffer.data(), static_cast<int>(buffer.second.buffer.size()), MPI_DOUBLE, 0, _subComms[counter], &_requests[counter]);
    if (err != MPI_SUCCESS){
      char *str= new char[MPI_MAX_ERROR_STRING];
      int len=-1;
      MPI_Error_string(err,str,&len);
      std::cout << "ERROR coupling::sendrecv::SendRecvBuffer::triggerBcasts(): Bcasting from rank " << thisRank << " on subcomm " << counter << " failed with error: " << str << std::endl;
      delete [] str;
      exit(EXIT_FAILURE);
    }
    counter++;
  }
#endif
}



template<class MacroscopicCell,unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::
triggerReceiving(const coupling::IndexConversion<dim>& indexConversion,
coupling::sendrecv::DataExchange<MacroscopicCell,dim>& dataExchange){
  #if (COUPLING_MD_PARALLEL==COUPLING_MD_YES)
  const unsigned int thisRank = indexConversion.getThisRank();

  // loop over all ranks (which are not this rank) and trigger MPI_Irecv
  // counter starts at _sendSize; recv-requests are in [_sendSize;_sendSize+_recvSize-1]
  int counter = _sendSize;
  for (typename std::map<unsigned int, BufferWithID>::iterator it = _receiveBuffer.begin(); it != _receiveBuffer.end(); it++){
    if ( (it->first) != thisRank){
      int err = MPI_Irecv( (it->second).buffer,(1+dataExchange.getDoublesPerCell())*(it->second).bufferSize,MPI_DOUBLE,(it->first),dataExchange.getTag(),MPI_COMM_WORLD,&_requests[counter]);
      if (err != MPI_SUCCESS){
        char *str=NULL; int len=-1;
        MPI_Error_string(err,str,&len);
        std::cout << "ERROR coupling::sendrecv::SendRecvBuffer::triggerReceiving(): Receiving on rank " << thisRank << " from " << it->first << " failed with error: " << str << std::endl;
        delete str;
        exit(EXIT_FAILURE);
      }
      counter++;
    }
  }
  #endif
}



template<class MacroscopicCell, unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::triggerReduce(
  const unsigned int thisRank
) {
#if(COUPLING_MD_PARALLEL == COUPLING_MD_YES)
  unsigned int counter = 0;
  for(std::pair<const unsigned int, BufferCollective> & buffer : _reduceBuffer) {

    if(buffer.second.nonRootRanks.empty()
      || (buffer.second.nonRootRanks.size() == 1 && *buffer.second.nonRootRanks.begin() == buffer.second.rootRank)
    ) {
      continue;
    }
    int rankInSubComm;
    MPI_Comm_rank(_subComms[counter], &rankInSubComm);

    double * sendBuffer, * recvBuffer;

    if(rankInSubComm == 0) {
      sendBuffer = static_cast<double *>(MPI_IN_PLACE);
      recvBuffer = buffer.second.buffer.data();
    } else {
      sendBuffer = buffer.second.buffer.data();
      recvBuffer = nullptr;
    }

    int err = MPI_Ireduce(sendBuffer, recvBuffer, static_cast<int>(buffer.second.buffer.size()),
                      MPI_DOUBLE, elementWiseSumOperation, 0, _subComms[counter], &_requests[counter]
    );
    if (err != MPI_SUCCESS){
      char * str = new char[MPI_MAX_ERROR_STRING];
      int len;
      MPI_Error_string(err,str,&len);
      std::cout << "ERROR coupling::sendrecv::SendRecvBuffer::triggerReduce(): Receiving Bcast on rank " << thisRank << " from " << buffer.first << " failed with error: " << str << std::endl;
      delete [] str;
      exit(EXIT_FAILURE);
    }
    counter += 1;
  }
#endif
}


template<class MacroscopicCell,unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::allocateRequests(const coupling::IndexConversion<dim>& indexConversion){
  #if (COUPLING_MD_PARALLEL==COUPLING_MD_YES)
  if (_requestsAllocated){
    std::cout << "ERROR coupling::SendReceiveBuffer::allocateRequests(): allocateRequests() called although allocation already took place!" << std::endl; exit(EXIT_FAILURE);
  }
  if (_requests!=NULL){delete []_requests; _requests=NULL;}
  const unsigned int thisRank = indexConversion.getThisRank();

  // determine number of MPI requests; no requests for on-rank operations
  _receiveSize=_receiveBuffer.size();
  _sendSize   =_sendBuffer.size();
  if (_sendBuffer.find(thisRank)!=_sendBuffer.end()){_sendSize--;}
  if (_receiveBuffer.find(thisRank)!=_receiveBuffer.end()){_receiveSize--;}
  // nop, that is no request allocations, if there are no MPI transfers
  if (_receiveSize+_sendSize==0){ return;}

  _requests = new MPI_Request[_receiveSize+_sendSize];
  if (_requests==NULL){std::cout << "ERROR coupling::SendReceiveBuffer::allocateRequests(): _requests==NULL!" << std::endl; exit(EXIT_FAILURE);}

  _requestsAllocated = true;
  #endif
}



template<class MacroscopicCell, unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::allocateBcastRequests(
  const unsigned int thisRank
) {
#if(COUPLING_MD_PARALLEL == COUPLING_MD_YES)

  if (_requestsAllocated){
    std::cout << "ERROR coupling::SendReceiveBuffer::allocateBcastRequests(): allocateRequests() called although allocation already took place!" << std::endl;
    exit(EXIT_FAILURE);
  }
  if (_requests!=nullptr){delete []_requests; _requests=nullptr;}

  // determine number of MPI requests
  _bcastSize = _bcastBuffer.size();

  // nop, that is no request allocations, if there are no MPI transfers
  if (_bcastSize==0){ return;}

  _requests = new MPI_Request[_bcastSize];
  if (_requests==nullptr){
    std::cout << "ERROR coupling::SendReceiveBuffer::allocateRequests(): _requests==NULL!" << std::endl;
    exit(EXIT_FAILURE);
  }

  // TODO better method for checking whether groups changed
  if(static_cast<unsigned int>(_bcastSize) == _subComms.size()) { return; }

  _subComms.resize(_bcastSize);
  _subGroups.resize(_bcastSize);

  MPI_Group group;
  MPI_Comm_group(MPI_COMM_WORLD, &group);


  // groups for receiving broadcasts
  unsigned int counter = 0;
  for( std::pair<const unsigned int, BufferCollective> & buffer : _bcastBuffer) {
    auto duplicate =buffer.second.nonRootRanks.find(buffer.second.rootRank);
    if(duplicate != buffer.second.nonRootRanks.end()) {
      buffer.second.nonRootRanks.erase(duplicate);
    }

    std::vector<int> groupRanks(buffer.second.nonRootRanks.size() + 1);
    std::copy(buffer.second.nonRootRanks.begin(), buffer.second.nonRootRanks.end(), std::next(groupRanks.begin()));
    groupRanks[0] = buffer.second.rootRank;

    MPI_Group_incl(group, static_cast<int>(groupRanks.size()), groupRanks.data(), &_subGroups[counter]);
    int err = MPI_Comm_create_group(MPI_COMM_WORLD, _subGroups[counter], static_cast<int>(buffer.first), &_subComms[counter]);
    if(err != MPI_SUCCESS) {
      char *str= new char[MPI_MAX_ERROR_STRING];
      int len=-1;
      MPI_Error_string(err,str,&len);
      std::cout << "ERROR coupling::sendrecv::SendRecvBuffer::allocateBcastRequests(): Unable to create communicator "
                << "on rank " << thisRank << " together with ranks [ ";
      for(int & item : groupRanks) { std::cout << item << " "; }
      std::cout << " ]" << std::endl;
      delete [] str;
      exit(EXIT_FAILURE);
    }

    //std::cout << thisRank << " : " << " allocated request for group " << groupRanks << " with tag " << buffer.first << std::endl;

    counter += 1;
  }

  _requestsAllocated = true;
#endif
}



template<class MacroscopicCell, unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::allocateReduceRequests(
const unsigned int thisRank
) {
#if(COUPLING_MD_PARALLEL == COUPLING_MD_YES)

  if (_requestsAllocated){
    std::cout << "ERROR coupling::SendReceiveBuffer::allocateReduceRequests(): allocateRequests() called although allocation already took place!" << std::endl;
    exit(EXIT_FAILURE);
  }
  if (_requests!=nullptr){delete []_requests; _requests=nullptr;}

  if(_reduceBuffer.empty()) { return; }
  // determine number of MPI requests
  _bcastSize = 0;
  for(std::pair<const unsigned int, BufferCollective> & buffer : _reduceBuffer) {
    const std::set<unsigned int> & nonRootRanks = buffer.second.nonRootRanks;
    if(!nonRootRanks.empty() && (nonRootRanks.size() > 1 || nonRootRanks.find(buffer.second.rootRank) == nonRootRanks.end())
    ) {
      _bcastSize += 1;
    }
  }

  // nop, that is no request allocations, if there are no MPI transfers
  if (_bcastSize==0){ return;}

  _requests = new MPI_Request[_bcastSize];
  if (_requests==nullptr){
    std::cout << "ERROR coupling::SendReceiveBuffer::allocateRequests(): _requests==NULL!" << std::endl;
    exit(EXIT_FAILURE);
  }

  // TODO better method for checking whether groups changed
  if(static_cast<unsigned int>(_bcastSize) == _subComms.size()) { return; }

  _subComms.resize(_bcastSize);
  _subGroups.resize(_bcastSize);

  MPI_Group group;
  MPI_Comm_group(MPI_COMM_WORLD, &group);

  // groups for receiving broadcasts
  unsigned int counter = 0;
  for( std::pair<const unsigned int, BufferCollective> & buffer : _reduceBuffer) {
    if(buffer.second.nonRootRanks.empty()
        || (buffer.second.nonRootRanks.size() == 1 && *buffer.second.nonRootRanks.begin() == buffer.second.rootRank)
    ) {
      continue;
    }

    // Sort all ranks into a vector, deleting possible duplicate of root rank
    std::vector<int> groupRanks(buffer.second.nonRootRanks.size() + 1);
    std::copy(buffer.second.nonRootRanks.begin(), buffer.second.nonRootRanks.end(), std::next(groupRanks.begin()));
    groupRanks[0] = buffer.second.rootRank;
    auto duplicate = std::find(std::next(groupRanks.begin()),groupRanks.end(), groupRanks[0]);
    if(duplicate != groupRanks.end()) {
      groupRanks.erase(duplicate);
    }

    // Create sub communicator of ranks
    MPI_Group_incl(group, static_cast<int>(groupRanks.size()), groupRanks.data(), &_subGroups[counter]);
    int err = MPI_Comm_create_group(MPI_COMM_WORLD, _subGroups[counter], static_cast<int>(buffer.first), &_subComms[counter]);
    if(err != MPI_SUCCESS) {
      char *str= new char[MPI_MAX_ERROR_STRING];
      int len=-1;
      MPI_Error_string(err,str,&len);
      std::cout << "ERROR coupling::sendrecv::SendRecvBuffer::allocateBcastRequests(): Unable to create communicator "
                << "on rank " << thisRank << " together with ranks [ ";
      for(int & item : groupRanks) { std::cout << item << " "; }
      std::cout << " ]" << std::endl;
      delete [] str;
      exit(EXIT_FAILURE);
    }

    std::cout << thisRank << " : " << " allocated request for group " << groupRanks << " with tag " << buffer.first << std::endl;

    counter += 1;
  }

  _requestsAllocated = true;
#endif
}





template<class MacroscopicCell,unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::
waitAllOperations(const coupling::IndexConversion<dim>& indexConversion){
  #if (COUPLING_MD_PARALLEL==COUPLING_MD_YES)
  // if no requests have been allocated, return immediately
  if (!_requestsAllocated && (_requests==NULL)){ return;}
  if (_requests==NULL){std::cout << "ERROR coupling::SendReceiveBuffer::waitAllOperations(): _requests==NULL!" << std::endl; exit(EXIT_FAILURE);}
  int err = MPI_Waitall (_sendSize+_receiveSize,_requests, MPI_STATUSES_IGNORE);
  if (err != MPI_SUCCESS){
    char *str=NULL; int len=-1;
    MPI_Error_string(err,str,&len);
    std::cout << "ERROR coupling::sendrecv::SendRecvBuffer::waitAllOperations(): Waiting failed with error: " << str << std::endl; delete str; exit(EXIT_FAILURE);
  }
  delete [] _requests; _requests = NULL;
  for(MPI_Comm & subcomm : _subComms) {
    if (subcomm != MPI_COMM_NULL) { MPI_Comm_free(&subcomm); subcomm = MPI_COMM_NULL; }
  }
  for(MPI_Group & subgroup : _subGroups) {
    if (subgroup != MPI_GROUP_NULL) { MPI_Group_free(&subgroup); subgroup = MPI_GROUP_NULL; }
  }
  _requestsAllocated = false;
  #endif
}



template<class MacroscopicCell,unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell,dim>::
waitAllBcasts(const coupling::IndexConversion<dim>& indexConversion){
#if (COUPLING_MD_PARALLEL==COUPLING_MD_YES)
  // if no requests have been allocated, return immediately
  if (!_requestsAllocated && (_requests==nullptr)){ return;}
  if (_requests==nullptr){std::cout << "ERROR coupling::SendReceiveBuffer::waitAllOperations(): _requests==NULL!" << std::endl; exit(EXIT_FAILURE);}
  int err = MPI_Waitall (_bcastSize,_requests, MPI_STATUSES_IGNORE);
  if (err != MPI_SUCCESS){
    char *str= new char[MPI_MAX_ERROR_STRING];
    int len=-1;
    MPI_Error_string(err,str,&len);
    delete [] str;
    exit(EXIT_FAILURE);
  }
  delete [] _requests; _requests = nullptr;
  /*for(MPI_Comm & subcomm : _subComms) {
    if (subcomm != MPI_COMM_NULL) { MPI_Comm_free(&subcomm); subcomm = MPI_COMM_NULL; }
  }
  for(MPI_Group & subgroup : _subGroups) {
    if (subgroup != MPI_GROUP_NULL) { MPI_Group_free(&subgroup); subgroup = MPI_GROUP_NULL; }
  }*/
  _requestsAllocated = false;
  #endif
}

template<class MacroscopicCell, unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell, dim>::allocateReduceBufferForReceiving(
const coupling::IndexConversion<dim> &indexConversion,
coupling::sendrecv::DataExchange<MacroscopicCell, dim> &dataExchange,
tarch::la::Vector<dim, unsigned int> globalVectorIndex
) {
  const unsigned int doublesPerCell = dataExchange.getDoublesPerCell()+1;


  // determine the ranks from which we need to receive information of this macroscopic cell
  const std::vector<unsigned int> sourceRanks = dataExchange.getSourceRanks(globalVectorIndex);
  const std::vector<unsigned int> targetRanks = dataExchange.getTargetRanks(globalVectorIndex);
  if(std::find(targetRanks.begin(), targetRanks.end(), indexConversion.getThisRank()) == targetRanks.end()) {
    // Nothing to do for this rank and subdomain!
    return;
  }
  /*if(indexConversion.getThisRank() == 15) {
    std::cout << globalVectorIndex << " <---> " << indexConversion.getGlobalCellIndex(globalVectorIndex) << " => "
              << sourceRanks << " ==> " << targetRanks << std::endl;
  }*/
  if(sourceRanks.empty()) { return; }
#if (COUPLING_MD_DEBUG==COUPLING_MD_YES)
  std::cout << "Rank " << indexConversion.getThisRank() << ": allocate receive buffers for cell " << globalVectorIndex << ";";
  std::cout << " associated source ranks: ";
  for (unsigned int i = 0; i < ranksSize; i++){ std::cout << ranks[i] << " ";} std::cout << std::endl;
#endif

  unsigned int intNumberProcesses = indexConversion.getNumberProcesses()[0];
  for(unsigned int i = 1; i < dim ; ++i) {
    intNumberProcesses *= indexConversion.getNumberProcesses()[i];
  }

  const unsigned int globalIndex = indexConversion.getGlobalCellIndex(globalVectorIndex);

  //unsigned int thisSubdomain = indexConversion.getThisRank() % intNumberProcesses;

  std::map<unsigned int, std::set<unsigned int> > subdomainMap;

  // Determine all sending ranks of this subdomain
  for(const unsigned int & rank : sourceRanks) {
    const unsigned int subdomain = rank % intNumberProcesses;
    const unsigned int key = indexConversion.getThisRank() * intNumberProcesses + subdomain;
    subdomainMap[key].insert(rank);
  }
  if(subdomainMap.empty()) { return; }

  // loop over source ranks
  for (std::pair<const unsigned int, std::set<unsigned int> > & subdomainEntry : subdomainMap){
    //if(sourceRank == indexConversion.getThisRank()) { continue; }
    unsigned int key = subdomainEntry.first;

    _reduceBuffer[key].rootRank = indexConversion.getThisRank();
    _reduceBuffer[key].nonRootRanks.insert(subdomainEntry.second.begin(), subdomainEntry.second.end());

    if(_reduceBuffer[key].cellIndices.find(globalIndex) != _reduceBuffer[key].cellIndices.end()) {
      // space for this cell already allocated, nothing to do!
      continue;
    }
    _reduceBuffer[key].cellIndices.insert(globalIndex);

    // increment buffer size and resize buffer
    const unsigned int firstPos = _reduceBuffer[key].buffer.size();
    _reduceBuffer[key].buffer.resize(_reduceBuffer[key].buffer.size() + doublesPerCell);
    // set all values to 0.0
    for (unsigned int j = firstPos; j < firstPos+doublesPerCell; j++){
      _reduceBuffer[key].buffer[j] = 0.0;
    }
  } // rank
}

template<class MacroscopicCell, unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell, dim>::writeToReduceBuffer(
const coupling::IndexConversion<dim> &indexConversion,
coupling::sendrecv::DataExchange<MacroscopicCell, dim> &dataExchange, const MacroscopicCell &cell,
tarch::la::Vector<dim, unsigned int> globalVectorIndex
) {
  const unsigned int doublesPerCell = dataExchange.getDoublesPerCell()+1;

  const unsigned int globalIndex = indexConversion.getGlobalCellIndex(globalVectorIndex);

  unsigned int intNumberProcesses = indexConversion.getNumberProcesses()[0];
  for(unsigned int d = 1; d < dim ; ++d) {
    intNumberProcesses *= indexConversion.getNumberProcesses()[d];
  }
  // determine the ranks which need to receive information of this macroscopic cell
  const std::vector<unsigned int> targetRanks = dataExchange.getTargetRanks(globalVectorIndex);
  const std::vector<unsigned int> sourceRanks = dataExchange.getSourceRanks(globalVectorIndex);

  if (targetRanks.empty()) {
    return;
  }

  // subdomain for this process
  unsigned int subdomainID = indexConversion.getThisRank() % intNumberProcesses;

  // determine unique subdomains which need to send this information
  // and associate them with nonRootRanks
  // note: this rank may also be part of nonRootRanks
  std::map<unsigned int, std::set<unsigned int> > subdomainMap;
  for(const unsigned int & targetRank : targetRanks) {

    unsigned int key = targetRank * intNumberProcesses + subdomainID;
    //subdomainMap[key].insert(indexConversion.getThisRank());
    for(const unsigned int & sourceRank : sourceRanks) {
      if(sourceRank % intNumberProcesses == subdomainID) {
        subdomainMap[key].insert(sourceRank);
      }
    }
  }

  // collect data for subdomains
  for(std::pair<const unsigned int, std::set<unsigned int>> & subdomain : subdomainMap) {
    unsigned int key = subdomain.first;

    // create new buffer if non-existent and insert
    // information about ranks
    _reduceBuffer[key].rootRank = (key - subdomainID) / intNumberProcesses;
    _reduceBuffer[key].nonRootRanks.insert(subdomain.second.begin(), subdomain.second.end());

    if(_reduceBuffer[key].cellIndices.find(globalIndex) != _reduceBuffer[key].cellIndices.end()) {
      // buffer already contains data of cell, nothing to do!
      continue;
    }
    _reduceBuffer[key].cellIndices.insert(globalIndex);

    // resize buffer for additional data
    const unsigned int firstPos = _reduceBuffer[key].buffer.size();
    _reduceBuffer[key].buffer.resize(_reduceBuffer[key].buffer.size() + doublesPerCell);

    // write global cell index and call to specialised function to read from macroscopic cell and write to send buffer
    _reduceBuffer[key].buffer[firstPos] = (double) globalIndex;

    dataExchange.readFromCell(&_reduceBuffer[key].buffer.data()[firstPos + 1], cell );
  }
}

template<class MacroscopicCell, unsigned int dim>
void coupling::sendrecv::SendReceiveBuffer<MacroscopicCell, dim>::readFromReduceBuffer(
const coupling::IndexConversion<dim> &indexConversion,
coupling::sendrecv::DataExchangeFromMD2Macro<dim> &dataExchange, MacroscopicCell &macroscopicCell,
const tarch::la::Vector<dim, unsigned int> globalVectorIndex
) {
  const unsigned int doublesPerCell = dataExchange.getDoublesPerCell()+1;
  const unsigned int globalIndex = indexConversion.getGlobalCellIndex(globalVectorIndex);

#if (COUPLING_MD_DEBUG==COUPLING_MD_YES)
  std::cout << "Rank " << indexConversion.getThisRank() << ": read cell " << globalVectorIndex << " (" << globalIndex << ") " << " from receive buffer" << std::endl;
#endif

  // loop over all received buffers
  for (typename std::map<unsigned int, BufferCollective>::const_iterator it = _reduceBuffer.begin(); it != _reduceBuffer.end(); it++){

    // loop over all received cells of this buffer
    const unsigned int bufferSize = it->second.buffer.size();
    for (unsigned int i = 0; i < bufferSize; i+=doublesPerCell){

      // determine global cell index for this macroscopic cell.
      // Due to the reduction, the cellIndices now look like cellIndice*commSize. We thus have to convert this value
      // back to its original.
      const auto thisIndex = (unsigned int) it->second.buffer[i] / it->second.nonRootRanks.size();

      // if this is the correct cell, write data into macroscopic cell and return
      if (thisIndex == globalIndex){
        // read receive buffer and write to macroscopic cell (use specialised function)
        dataExchange.writeToCell( &(it->second.buffer.data()[i+1]), macroscopicCell );
        return;
      }
    } // loop over buffer
  } // loop over all buffers
}
